{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Four Types of Forward Simulation Operators to Consider in Automatic Differentiation\n",
    "\n",
    "All numerical simulations can be decomposed into operators that are chained together. These operators range from a simple arithmetic operation such as addition or multiplication, to more sophisticated computation such as solving a linear system. Automatic differentiation relies on the differentiation of those operators and integrates them with chain rules. Therefore, it is very important for us to study the basic types of existing operators. \n",
    "\n",
    "![Operators](sim.png)\n",
    "\n",
    "In this tutorial, a operator is defined as a numerical procedure that accepts a parameter called *input*, $x$, and turns out a parameter called *ouput*, $y=f(x)$. For reverse mode automatic differentiation, besides evaluating $f(x)$, we need also to compute $\\frac{\\partial J}{\\partial x}$ given $\\frac{\\partial J}{\\partial y}$ where $J$ is a functional of $y$. \n",
    "\n",
    "Note  the operator $y=f(x)$ may be implicit in the sense that $f$ is not given directly. In general, we can write the relationship between $x$ and $y$ as $F(x,y)=0$. The operator is *well-defined* if for given $x$, there exists one and only one $y$ such that $F(x,y)=0$. \n",
    "\n",
    "For automatic differentiation, besides the well-definedness of $F$, we also require that we can compute $\\frac{\\partial J}{\\partial x}$ given $\\frac{\\partial J}{\\partial y}$. It is easy to see that\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial x} = -\\frac{\\partial J}{\\partial y}F_y^{-1}F_x\n",
    "$$\n",
    "Therefore, we call an operator $F$ is *well-posed* if $F_y^{-1}$ exists. \n",
    "\n",
    "All operators can be classified into four types based on the linearity and explicitness\n",
    "\n",
    "* **Linear and explicit**\n",
    "  This type of operators has the form \n",
    "  $$\n",
    "  y = Ax\n",
    "  $$\n",
    "  where $A$ is a matrix. In this case, \n",
    "  $$\n",
    "  F(x,y) = Ax-y\n",
    "  $$\n",
    "  and therefore \n",
    "  $$\n",
    "  \\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}A\n",
    "  $$\n",
    "\n",
    "* **Nonlinear and explicit**\n",
    "  In this case, we have \n",
    "  $$\n",
    "  y = F(x)\n",
    "  $$\n",
    "  where $F$ is explicitly given. We have\n",
    "  $$\n",
    "  F(x,y) = F(x)-y\\Rightarrow \\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y} F_x(x)\n",
    "  $$\n",
    "  \n",
    "- **Linear and implicit**\n",
    "  In this case \n",
    "  $$\n",
    "  Ay = x\n",
    "  $$\n",
    "  We have $F(x,y) = x-Ay$ and \n",
    "  $$\n",
    "  \\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}A^{-1}\n",
    "  $$\n",
    "\n",
    "- **Nonlinear and implicit**\n",
    "\n",
    "  In this case $F(x,y)=0$ and the corresponding gradient is \n",
    "  $$\n",
    "  \\frac{\\partial J}{\\partial x} = -\\frac{\\partial J}{\\partial y}F_y^{-1}F_x\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In `TensorFlow` it is easy to implement linear/nonlinear and explicit operators and takes reasonable effor for linear and implicit operators. However, it is challenging to implement nonlinear and implicit method. We provide a solution by marrying `PyTorch` and `TensorFlow` [here](https://github.com/kailaix/ADCME.jl/tree/master/examples/md/pytorch.pdf)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
