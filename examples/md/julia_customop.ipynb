{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling Julia from TensorFlow\n",
    "\n",
    "In scientific and engineering applications, the operators provided by `TensorFlow` are not sufficient for high performance computing. In addition, constraining oneself to `TensorFlow` environment sacrifices the powerful scientific computing ecosystems provided by other languages such as `Julia` and `Python`. For example, one might want to code a finite volume method for a sophisticated fluid dynamics problem; it is hard to have the flexible syntax to achieve this goal, obtain performance boost from existing fast solvers such as AMG, and benefit from many other third-party packages within `TensorFlow`. This motivates us to find a way to \"plugin\" custom operators to `TensorFlow`.\n",
    "\n",
    "\n",
    "\n",
    "We have already introduced how to incooperate `C++` custom operators.  For many researchers, they usually prototype the solvers in a high level language such as MATLAB, Julia or Python. To enjoy the parallelism and automatic differentiation feature of `TensorFlow`, they need to port them into `C/C++`. However, this is also cumbersome sometimes, espeically the original solvers depend on many packages in the high-level language. \n",
    "\n",
    "\n",
    "\n",
    "We solve this problem by incorporating `Julia` functions directly into `TensorFlow`. That is, for any `Julia` functions, we can immediately convert it to a `TensorFlow` operator. At runtime, when this operator is executed, the corresponding `Julia` function is executed. That implies we have the `Julia` speed. Most importantly, the function is perfectly compitable with the native `Julia` environment; third-party packages, global variables, nested functions, etc. all work smoothly. Since `Julia` has the ability to call other languages in a quite elegant and simple manner, such as `C/C++`, `Python`, `R`, `Java`, this means it is possible to incoporate packages/codes from any supported languages into `TensorFlow` ecosystem. We need to point out that in `TensorFlow`, `tf.numpy_function` can be used to convert a `Python` function to a `TensorFlow` operator. However, in the runtime, the speed for this operator falls back to `Python` (or `numpy` operation for related parts). This is a drawback. \n",
    "\n",
    "\n",
    "\n",
    "The key for implementing the mechanism is embedding `Julia` in `C++`. Still we need to create a `C++` dynamic library for `TensorFlow`. However, the library is only an interface for invoking `Julia` code. At runtime, `jl_get_function` is called to search for the related function in the main module. `C++` arrays, which include all the relavant data, are passed to this function through `jl_call`. It requires routine convertion from `C++` arrays to `Julia` array interfaces `jl_array_t*`. However, those bookkeeping tasks are programatic and possibly will be automated in the future. Afterwards,`Julia` returns the result to `C++` and thereafter the data are passed to the next operator. \n",
    "\n",
    "\n",
    "\n",
    "There are two caveats in the implementation. The first is that due to GIL of Python, we must take care of the thread lock while interfacing with `Julia`. This was done by putting a guard around th e`Julia` interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "c"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "PyGILState_STATE py_threadstate;\n",
    "py_threadstate = PyGILState_Ensure();\n",
    "// code here \n",
    "PyGILState_Release(py_threadstate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is the memory mangement of `Julia` arrays. This was done by defining gabage collection markers explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "jl_value_t **args;\n",
    "JL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\n",
    "args[0] = ...\n",
    "args[1] = ...\n",
    "# do something\n",
    "JL_GC_POP();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is remarkable and puts together one of the best langages in scientific computing and that in machine learning. The work that can be built on `ADCME` is enormous and significantly reduce the development time. \n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Here we present a simple example. Suppose we want to compute the Jacobian of a two layer neural network $\\frac{\\partial y}{\\partial x}$\n",
    "\n",
    "$$y = W_2\\tanh(W_1x+b_1)+b_2$$\n",
    "\n",
    "where $x, b_1, b_2, y\\in \\mathbb{R}^{10}$, $W_1, W_2\\in \\mathbb{R}^{100}$. In `TensorFlow`, this can be done by computing the gradients $\\frac{\\partial y_i}{\\partial x}$ for each $i$. In `Julia`, we can use `ForwardDiff` to do it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "function twolayer(J, x, w1, w2, b1, b2)\n",
    "    f = x -> begin\n",
    "        w1 = reshape(w1, 10, 10)\n",
    "        w2 = reshape(w2, 10, 10)\n",
    "        z = w2*tanh.(w1*x+b1)+b2\n",
    "    end\n",
    "    J[:] = ForwardDiff.jacobian(f, x)[:]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a custom operator, we first generate a wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "using ADCME\n",
    "mkdir(\"TwoLayer\")\n",
    "cd(\"TwoLayer\")\n",
    "customop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify `custom_op.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoLayer\n",
    "double x(?)\n",
    "double w1(?)\n",
    "double b1(?)\n",
    "double w2(?)\n",
    "double b2(?)\n",
    "double y(?) -> output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "customop(;julia=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three files are generated`CMakeLists.txt`, `TwoLayer.cpp` and `gradtest.jl`. Now create a new file `TwoLayer.h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "c++"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "#include \"julia.h\"\n",
    "#include \"Python.h\"\n",
    "\n",
    "void forward(double *y, const double *x, const double *w1, const double *w2, const double *b1, const double *b2, int n){\n",
    "    PyGILState_STATE py_threadstate;\n",
    "    py_threadstate = PyGILState_Ensure();\n",
    "    jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);\n",
    "    jl_value_t **args;\n",
    "    JL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects\n",
    "    args[0] = (jl_value_t*)jl_ptr_to_array_1d(array_type, y, n*n, 0);\n",
    "    args[1] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(x), n, 0);\n",
    "    args[2] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w1), n*n, 0);\n",
    "    args[3] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(w2), n*n, 0);\n",
    "    args[4] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b1), n, 0);\n",
    "    args[5] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast<double*>(b2), n, 0);\n",
    "    auto fun = jl_get_function(jl_main_module, \"twolayer\");\n",
    "    jl_call(fun, args, 6);\n",
    "    JL_GC_POP();\n",
    "    if (jl_exception_occurred())\n",
    "        printf(\"%s \\n\", jl_typeof_str(jl_exception_occurred()));\n",
    "    PyGILState_Release(py_threadstate);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the codes have been explanined except `jl_ptr_to_array_1d`. This function generates a `Julia` array wrapper from `C++` arrays. The last argument `0` indicates that `Julia` is not responsible for gabage collection. `TwoLayer.cpp` should also be modified according to [https://github.com/kailaix/ADCME.jl/blob/master/examples/twolayer_jacobian/TwoLayer.cpp](https://github.com/kailaix/ADCME.jl/blob/master/examples/twolayer_jacobian/TwoLayer.cpp).\n",
    "\n",
    "\n",
    "\n",
    "Finally, we can test in `gradtest.jl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "two_layer = load_op(\"build/libTwoLayer\", \"two_layer\")\n",
    "\n",
    "\n",
    "w1 = rand(100)\n",
    "w2 = rand(100)\n",
    "b1 = rand(10)\n",
    "b2 = rand(10)\n",
    "x = rand(10)\n",
    "J = rand(100)\n",
    "twolayer(J, x, w1, w2, b1, b2)\n",
    "\n",
    "y = two_layer(constant(x), constant(w1), constant(b1), constant(w2), constant(b2))\n",
    "sess = Session(); init(sess)\n",
    "J0 = run(sess, y)\n",
    "@show norm(J-J0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Sheet\n",
    "\n",
    "For implementation reference, see [Reference Sheet](https://github.com/kailaix/ADCME.jl/blob/master/examples/md/customop_reference_sheet.md)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
