{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Meets PyTorch: Using PyTorch to Create TensorFlow Custom Operators\n",
    "\n",
    "## Why do we need `PyTorch`?\n",
    "\n",
    "In [another tutorial](https://github.com/kailaix/ADCME.jl/tree/master/examples/md/four_types.pdf), we talked about four types of forward simulation operators. There is no general way to code nonlinear implicit operators in `TensorFlow` and therefore we resort to custom operators. However, this brings another problem: we need to implement the backward operator, i.e.,\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial x} = -\\frac{\\partial J}{\\partial y}F_y^{-1}F_x\n",
    "$$\n",
    "This requires us to solve a linear system where the coefficient matrix is the Jacobian $F_y$. Also, we need to compute the matrix vector production where the matrix is the Jacobian $F_x$. The problem also arises when one tries to implement custom operators for explicit nonlinear operator, where the backward operator is \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y} F_x(x)\n",
    "$$\n",
    "\n",
    "## Step-by-Step Instruction\n",
    "\n",
    "In this tutorial, we show how to implement the backward operator without deriving the Jacobian by leveraging the `PyTorch` `Aten` library. Again, for concreteness, we assume that the nonlinear implicit operator is\n",
    "$$\n",
    "F(x,y) = x^2 - y^3/(1+x)\n",
    "$$\n",
    "The operator can be written in the explicit form for verification\n",
    "$$\n",
    "y = (x^2(1+x))^{\\frac13}\n",
    "$$\n",
    "\n",
    "* **Step 1: Computing **$g = \\frac{\\partial J}{\\partial y}F_y^{-1}$\n",
    "\n",
    "This step requires solving a linear system \n",
    "$$\n",
    "F_y u =\\frac{\\partial J}{\\partial y}\n",
    "$$\n",
    "To solve the linear system, we can apply an iterative solver such as GMRES. This requires us to be able to do matrix vector production. This can be cleverly done in `PyTorch` by letting $y$ be a `Variable`, $x,u$ be non-trainable tensors, and differentiate $F(x,y)\\cdot u$ with respect to $y$, and we obtained $F_yu$ directly. \n",
    "\n",
    "* **Step 2: Computing** $gF_x$ \n",
    "\n",
    "Similar to Step 1, this can be done by treating $x$ as `Variable` while $y,u$ as non-trainable tensors. Differentiation with respect to $x$ gives the desired results. \n",
    "\n",
    "For a full working script, see [here](https://github.com/kailaix/ADCME.jl/tree/master/examples/torch/laexample.cpp)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
